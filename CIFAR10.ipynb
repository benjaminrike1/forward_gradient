{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## CIFAR-10\n",
        "\n",
        "This notebook contains our experiment with forward gradient vs. backproagation for the CIFAR-10 dataset. "
      ],
      "metadata": {
        "id": "ihhfNQu1mZ7b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup"
      ],
      "metadata": {
        "id": "J260V0OmmoCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run once\n",
        "# CPU only: !pip install torch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0 -f https://download.pytorch.org/whl/cpu/torch_stable.html\n",
        "!pip install --pre torch -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html --upgrade\n",
        "!pip install functorch\n",
        "print(\"--> Restarting colab instance\") \n",
        "get_ipython().kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "E4zZS3LNmTrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/benjaminrike1/forward_gradient"
      ],
      "metadata": {
        "id": "Xrs8UKgDmYc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd forward_gradient"
      ],
      "metadata": {
        "id": "VBbL5Bm3oWTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRUNfjWZlx61"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "import functorch as ft\n",
        "\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from optim_functions import beale, rosenbrock\n",
        "from helpers import optimize\n",
        "from plot_helpers import plot_loss, plot_countour, plot_contour2\n",
        "from loss import functional_xent, softmax, clamp_probs, _xent\n",
        "from optimizers import ForwardSGD\n",
        "from models import Net, ConvNet, LogisticRegression\n",
        "\n",
        "torch.manual_seed(0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CIFAR-10"
      ],
      "metadata": {
        "id": "oIf0rMvOtz6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "mnist_train = torchvision.datasets.CIFAR10(\n",
        "    '/tmp/data',\n",
        "    train=True, \n",
        "    download=True, \n",
        "    transform=transform\n",
        ")\n",
        "train_data_loader = torch.utils.data.DataLoader(mnist_train, \n",
        "                                          batch_size=64, \n",
        "                                          shuffle=True)\n",
        "\n",
        "mnist_test = torchvision.datasets.CIFAR10(\n",
        "    '/tmp/data',\n",
        "    train=False, \n",
        "    download=True, \n",
        "    transform=transform\n",
        ")\n",
        "test_data_loader = torch.utils.data.DataLoader(mnist_test, \n",
        "                                              batch_size=64,\n",
        "                                              shuffle=True)"
      ],
      "metadata": {
        "id": "fFFyHjrRtzG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network"
      ],
      "metadata": {
        "id": "xnoi5RW9qm5U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SGD"
      ],
      "metadata": {
        "id": "_qH5VOIWwAQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "TTxOWIqgqyDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward gradient:"
      ],
      "metadata": {
        "id": "8GhqG9jAqy03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net().to(device) # defining net\n",
        "\n",
        "# making the net functional to run the code in functorch\n",
        "# for evaluating the Jacobian-vector product\n",
        "func, params = ft.make_functional(net)\n",
        "\n",
        "# removing requires gradient as it will not be used\n",
        "# for the forward AD\n",
        "for param in params:\n",
        "    param.requires_grad_(False)\n",
        "\n",
        "# defining our optimizer\n",
        "opt = ForwardSGD(func, functional_xent, params, lr=2e-4, momentum = False, decay=1e-5)\n",
        "\n",
        "# running the code for e epochs\n",
        "losses_fwd = []\n",
        "epochs = 50\n",
        "test_losses_fwd = []\n",
        "for e in range(epochs):\n",
        "  # training\n",
        "  for i, (image, label) in enumerate(train_data_loader):\n",
        "    image, label = image.to(device), label.to(device)\n",
        "    params, loss = opt.step(image, label)\n",
        "    losses_fwd.append(loss.item())\n",
        "  # evaluating on the test set\n",
        "  for i, (image, label) in enumerate(test_data_loader):\n",
        "    batch_loss = []\n",
        "    with torch.no_grad():\n",
        "      image, label = image.to(device), label.to(device)\n",
        "      test_loss = functional_xent(func, params, image, label)\n",
        "      batch_loss.append(test_loss.item())\n",
        "    test_losses.append(np.mean(batch_loss))\n",
        "    print(f\"Test loss in epoch {i+1}: {np.mean(batch_loss)}\")"
      ],
      "metadata": {
        "id": "D7BcefWeqh_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backpropagation:"
      ],
      "metadata": {
        "id": "G4WpUezcqr4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss() # loss function\n",
        "net = Net().to(device) # defining net\n",
        "backprop = torch.optim.SGD(net.parameters(), lr=2e-4, weight_decay=1e-4) # normal SGD in torch\n",
        "\n",
        "epochs=50\n",
        "\n",
        "# storing losses\n",
        "losses = []\n",
        "test_losses = []\n",
        "for epoch in range(epochs):\n",
        "  # going over training set in batches\n",
        "  for i, (image, label) in enumerate(train_data_loader):\n",
        "    image, label = image.to(device), label.to(device)\n",
        "    backprop.zero_grad()\n",
        "    outputs = net(image)\n",
        "    loss = criterion(outputs, label)\n",
        "    loss.backward()\n",
        "    backprop.step()\n",
        "    losses.append(loss.item())\n",
        "  for i, (image, label) in enumerate(test_data_loader):\n",
        "    batch_loss = []\n",
        "    with torch.no_grad():\n",
        "      image, label = image.to(device), label.to(device)\n",
        "      test_loss = criterion(net(image), label)\n",
        "      batch_loss.append(test_loss.item())\n",
        "    test_losses.append(np.mean(batch_loss))\n",
        "    print(f\"Test loss in epoch {i+1}: {np.mean(batch_loss)}\")"
      ],
      "metadata": {
        "id": "Id8vMur6qrcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparing results"
      ],
      "metadata": {
        "id": "j7wyih5wrNzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,2,figsize=(16,8))\n",
        "\n",
        "ax[0].plot(losses, color='r', label=\"Backprop\", alpha=.7)\n",
        "ax[0].set_xlabel(\"Iterations\")\n",
        "ax[0].set_ylabel(\"Loss\")\n",
        "ax[0].plot(losses_fwd, color='b', label='Forward gradient', alpha=.7)\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(test_losses, color='r', label=\"Backprop\", alpha=.7)\n",
        "ax[1].set_xlabel(\"Epochs\")\n",
        "ax[1].set_ylabel(\"Loss\")\n",
        "ax[1].plot(test_losses_fwd, color='b', label='Forward gradient', alpha=.7)\n",
        "ax[1].legend()"
      ],
      "metadata": {
        "id": "d1Az9_GcrQC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning rate optimization\n",
        "\n",
        "The final search for learning rate is in a quite small interval as we earlier tried a wider search, but wanted to reduce the width to find a better optimum."
      ],
      "metadata": {
        "id": "ixUBfiPVvy3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = np.logspace(-5, -3, 3)\n",
        "decays = np.logspace(-6, -4, 3)\n",
        "\n",
        "for gamma in learning_rates:\n",
        "  for lambda_ in decays:\n",
        "    net = Net().to(device) # defining net\n",
        "\n",
        "    # making the net functional to run the code in functorch\n",
        "    # for evaluating the Jacobian-vector product\n",
        "    func, params = ft.make_functional(net)\n",
        "\n",
        "    # removing requires gradient as it will not be used\n",
        "    # for the forward AD\n",
        "    for param in params:\n",
        "        param.requires_grad_(False)\n",
        "\n",
        "    # defining our optimizer\n",
        "    opt = ForwardSGD(func, functional_xent, params, lr=gamma, momentum = False, decay=lambda_)\n",
        "    # running the code for e epochs\n",
        "    epochs = 10\n",
        "    test_losses_fwd = []\n",
        "    for e in range(epochs):\n",
        "      # training\n",
        "      for i, (image, label) in enumerate(train_data_loader):\n",
        "        image, label = image.to(device), label.to(device)\n",
        "        params, loss = opt.step(image, label)\n",
        "      # evaluating on the test set\n",
        "      for i, (image, label) in enumerate(val_data_loader):\n",
        "        batch_loss = []\n",
        "        with torch.no_grad():\n",
        "          image, label = image.to(device), label.to(device)\n",
        "          val_loss = functional_xent(func, params, image, label)\n",
        "          batch_loss.append(val_loss.item())\n",
        "        test_losses.append(np.mean(batch_loss))\n",
        "        print(f\"Test loss in epoch {i+1}: {np.mean(batch_loss)}\")"
      ],
      "metadata": {
        "id": "b1G1P4OWwEk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conv Net"
      ],
      "metadata": {
        "id": "BKD8xerQrQ-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward gradient:"
      ],
      "metadata": {
        "id": "sEEwtxYCrW5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = ConvNet().to(device) # defining net\n",
        "\n",
        "# making the net functional to run the code in functorch\n",
        "# for evaluating the Jacobian-vector product\n",
        "func, params = ft.make_functional(net)\n",
        "\n",
        "# removing requires gradient as it will not be used\n",
        "# for the forward AD\n",
        "for param in params:\n",
        "    param.requires_grad_(False)\n",
        "\n",
        "# defining our optimizer\n",
        "opt = ForwardSGD(func, functional_xent, params, lr=2e-4, momentum = False, decay=1e-5)\n",
        "\n",
        "# running the code for e epochs\n",
        "losses_fwd = []\n",
        "epochs = 50\n",
        "test_losses = []\n",
        "for e in range(epochs):\n",
        "  # training\n",
        "  for i, (image, label) in enumerate(train_data_loader):\n",
        "    image, label = image.to(device), label.to(device)\n",
        "    params, loss = opt.step(image, label)\n",
        "    losses_fwd.append(loss.item())\n",
        "  # evaluating on the test set\n",
        "  for i, (image, label) in enumerate(test_data_loader):\n",
        "    batch_loss = []\n",
        "    with torch.no_grad():\n",
        "      image, label = image.to(device), label.to(device)\n",
        "      test_loss = functional_xent(func, params, image, label)\n",
        "      batch_loss.append(test_loss.item())\n",
        "    test_losses.append(np.mean(batch_loss))\n",
        "    print(f\"Test loss in epoch {i+1}: {np.mean(batch_loss)}\")"
      ],
      "metadata": {
        "id": "gfOzwDiqrX9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backpropagation:"
      ],
      "metadata": {
        "id": "byonpp14rafx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss() # loss function\n",
        "net = ConvNet().to(device) # defining net\n",
        "backprop = torch.optim.SGD(net.parameters(), lr=2e-4, weight_decay=1e-4) # normal SGD in torch\n",
        "\n",
        "epochs=50\n",
        "\n",
        "# storing losses\n",
        "losses = []\n",
        "test_losses = []\n",
        "for epoch in range(epochs):\n",
        "  # going over training set in batches\n",
        "  for i, (image, label) in enumerate(train_data_loader):\n",
        "    image, label = image.to(device), label.to(device)\n",
        "    backprop.zero_grad()\n",
        "    outputs = net(image)\n",
        "    loss = criterion(outputs, label)\n",
        "    loss.backward()\n",
        "    backprop.step()\n",
        "    losses.append(loss.item())\n",
        "  for i, (image, label) in enumerate(test_data_loader):\n",
        "    batch_loss = []\n",
        "    with torch.no_grad():\n",
        "      image, label = image.to(device), label.to(device)\n",
        "      test_loss = criterion(net(image), label)\n",
        "      batch_loss.append(test_loss.item())\n",
        "    test_losses.append(np.mean(batch_loss))\n",
        "    print(f\"Test loss in epoch {i+1}: {np.mean(batch_loss)}\")\n",
        "    "
      ],
      "metadata": {
        "id": "cNWURG-mrcCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparing results"
      ],
      "metadata": {
        "id": "R_fVJq9ardZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,2,figsize=(16,8))\n",
        "\n",
        "ax[0].plot(losses, color='r', label=\"Backprop\", alpha=.7)\n",
        "ax[0].set_xlabel(\"Iterations\")\n",
        "ax[0].set_ylabel(\"Loss\")\n",
        "ax[0].plot(losses_fwd, color='b', label='Forward gradient', alpha=.7)\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(test_losses, color='r', label=\"Backprop\", alpha=.7)\n",
        "ax[1].set_xlabel(\"Epochs\")\n",
        "ax[1].set_ylabel(\"Loss\")\n",
        "ax[1].plot(test_losses_fwd, color='b', label='Forward gradient', alpha=.7)\n",
        "ax[1].legend()"
      ],
      "metadata": {
        "id": "0blf7s25retQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
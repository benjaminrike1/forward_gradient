{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "JmBQQnpWoJrd",
        "Vvt5q3XaqbUc",
        "BKD8xerQrQ-y"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MNIST\n",
        "\n",
        "This notebook contains our experiements comparing forward gradient and backpropagation with neural networks and Convolutional nets for the MNIST dataset"
      ],
      "metadata": {
        "id": "SpgPiFcBm1hX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup"
      ],
      "metadata": {
        "id": "JmBQQnpWoJrd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnA7qJ4F78Gr",
        "outputId": "3aa3d717-6b28-4a3d-e138-00ba07de78ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cpu/torch-1.13.0.dev20220610%2Bcpu-cp37-cp37m-linux_x86_64.whl (190.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 190.7 MB 59 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.13.0.dev20220610+cpu which is incompatible.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.13.0.dev20220610+cpu which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.13.0.dev20220610+cpu which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.13.0.dev20220610+cpu\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting functorch\n",
            "  Downloading functorch-0.1.1-cp37-cp37m-manylinux1_x86_64.whl (21.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.4 MB 1.5 MB/s \n",
            "\u001b[?25hCollecting torch<1.12,>=1.11\n",
            "  Downloading torch-1.11.0-cp37-cp37m-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 750.6 MB 11 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<1.12,>=1.11->functorch) (4.2.0)\n",
            "Installing collected packages: torch, functorch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.0.dev20220610+cpu\n",
            "    Uninstalling torch-1.13.0.dev20220610+cpu:\n",
            "      Successfully uninstalled torch-1.13.0.dev20220610+cpu\n"
          ]
        }
      ],
      "source": [
        "# Run once\n",
        "# CPU only: !pip install torch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0 -f https://download.pytorch.org/whl/cpu/torch_stable.html\n",
        "!pip install --pre torch -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html --upgrade\n",
        "!pip install functorch\n",
        "print(\"--> Restarting colab instance\") \n",
        "get_ipython().kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/benjaminrike1/forward_gradient"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w16dKpsQbsXH",
        "outputId": "8546c8c5-9235-4782-eadd-2190750bb803"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'forward_gradient'...\n",
            "remote: Enumerating objects: 59, done.\u001b[K\n",
            "remote: Counting objects: 100% (59/59), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 59 (delta 19), reused 39 (delta 9), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (59/59), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd forward_gradient"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7a4RrKbmuKQ",
        "outputId": "44287b7f-efb1-4fa6-8370-0b6e43908690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/forward_gradient\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngzkoQhx8SHI",
        "outputId": "367c1bfe-73ab-4cae-d049-6581e71eb619"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
            "  warn(f\"Failed to load image Python extension: {e}\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f382b01ea90>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "import functorch as ft\n",
        "\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from optim_functions import beale, rosenbrock\n",
        "from helpers import optimize\n",
        "from plot_helpers import plot_loss, plot_countour, plot_contour2\n",
        "from loss import functional_xent, softmax, clamp_probs, _xent\n",
        "from optimizers import ForwardSGD\n",
        "from models import Net, ConvNet, LogisticRegression\n",
        "\n",
        "torch.manual_seed(0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST"
      ],
      "metadata": {
        "id": "Vvt5q3XaqbUc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the data from torchvision."
      ],
      "metadata": {
        "id": "Vy2UKEhnqexi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "mnist_train = torchvision.datasets.MNIST(\n",
        "    '/tmp/data',\n",
        "    train=True, \n",
        "    download=True, \n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "train, val = torch.utils.data.random_split(mnist_train, [50000, 10000])\n",
        "\n",
        "train_data_loader = torch.utils.data.DataLoader(train, \n",
        "                                          batch_size=64, \n",
        "                                          shuffle=True)\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(val, \n",
        "                                          batch_size=64, \n",
        "                                          shuffle=True)\n",
        "\n",
        "mnist_test = torchvision.datasets.MNIST(\n",
        "    '/tmp/data',\n",
        "    train=False, \n",
        "    download=True, \n",
        "    transform=transform\n",
        ")\n",
        "test_data_loader = torch.utils.data.DataLoader(mnist_test, \n",
        "                                              batch_size=64,\n",
        "                                              shuffle=True)"
      ],
      "metadata": {
        "id": "fnfSMWT8qajP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network"
      ],
      "metadata": {
        "id": "xnoi5RW9qm5U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SGD"
      ],
      "metadata": {
        "id": "_qH5VOIWwAQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "TTxOWIqgqyDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward gradient:"
      ],
      "metadata": {
        "id": "8GhqG9jAqy03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net().to(device) # defining net\n",
        "\n",
        "# making the net functional to run the code in functorch\n",
        "# for evaluating the Jacobian-vector product\n",
        "func, params = ft.make_functional(net)\n",
        "\n",
        "# removing requires gradient as it will not be used\n",
        "# for the forward AD\n",
        "for param in params:\n",
        "    param.requires_grad_(False)\n",
        "\n",
        "# defining our optimizer\n",
        "opt = ForwardSGD(func, functional_xent, params, lr=2e-4, momentum = False, decay=1e-5)\n",
        "\n",
        "# running the code for e epochs\n",
        "losses_fwd = []\n",
        "epochs = 50\n",
        "test_losses_fwd = []\n",
        "for e in range(epochs):\n",
        "  # training\n",
        "  for i, (image, label) in enumerate(train_data_loader):\n",
        "    image, label = image.to(device), label.to(device)\n",
        "    params, loss = opt.step(image, label)\n",
        "    losses_fwd.append(loss.item())\n",
        "    # evaluating on the test set\n",
        "    for i, (image, label) in enumerate(test_data_loader):\n",
        "      batch_loss = []\n",
        "      with torch.no_grad():\n",
        "        image, label = image.to(device), label.to(device)\n",
        "        test_loss = functional_xent(func, params, image, label)\n",
        "        batch_loss.append(test_loss.item())\n",
        "      test_losses.append(np.mean(batch_loss))\n",
        "      print(f\"Test loss in epoch {i+1}: {np.mean(batch_loss)}\")"
      ],
      "metadata": {
        "id": "D7BcefWeqh_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backpropagation:"
      ],
      "metadata": {
        "id": "G4WpUezcqr4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss() # loss function\n",
        "net = Net().to(device) # defining net\n",
        "backprop = torch.optim.SGD(net.parameters(), lr=2e-4, weight_decay=1e-4) # normal SGD in torch\n",
        "\n",
        "epochs=50\n",
        "\n",
        "# storing losses\n",
        "losses = []\n",
        "test_losses = []\n",
        "for epoch in range(epochs):\n",
        "  # going over training set in batches\n",
        "  for i, (image, label) in enumerate(train_data_loader):\n",
        "    image, label = image.to(device), label.to(device)\n",
        "    backprop.zero_grad()\n",
        "    outputs = net(image)\n",
        "    loss = criterion(outputs, label)\n",
        "    loss.backward()\n",
        "    backprop.step()\n",
        "    losses.append(loss.item())\n",
        "    for i, (image, label) in enumerate(test_data_loader):\n",
        "      batch_loss = []\n",
        "      with torch.no_grad():\n",
        "        image, label = image.to(device), label.to(device)\n",
        "        test_loss = criterion(net(image), label)\n",
        "        batch_loss.append(test_loss.item())\n",
        "      test_losses.append(np.mean(batch_loss))\n",
        "      print(f\"Test loss in epoch {i+1}: {np.mean(batch_loss)}\")"
      ],
      "metadata": {
        "id": "Id8vMur6qrcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparing results"
      ],
      "metadata": {
        "id": "j7wyih5wrNzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,2,figsize=(16,8))\n",
        "\n",
        "ax[0].plot(losses, color='r', label=\"Backprop\", alpha=.7)\n",
        "ax[0].set_xlabel(\"Iterations\")\n",
        "ax[0].set_ylabel(\"Loss\")\n",
        "ax[0].plot(losses_fwd, color='b', label='Forward gradient', alpha=.7)\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(test_losses, color='r', label=\"Backprop\", alpha=.7)\n",
        "ax[1].set_xlabel(\"Epochs\")\n",
        "ax[1].set_ylabel(\"Loss\")\n",
        "ax[1].plot(test_losses_fwd, color='b', label='Forward gradient', alpha=.7)\n",
        "ax[1].legend()"
      ],
      "metadata": {
        "id": "d1Az9_GcrQC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning rate optimization\n",
        "\n",
        "The final search for learning rate is in a quite small interval as we earlier tried a wider search, but wanted to reduce the width to find a better optimum."
      ],
      "metadata": {
        "id": "ixUBfiPVvy3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = np.logspace(-5, -3, 3)\n",
        "decays = np.logspace(-6, -4, 3)\n",
        "\n",
        "for gamma in learning_rates:\n",
        "  for lambda_ in decays:\n",
        "    net = Net().to(device) # defining net\n",
        "\n",
        "    # making the net functional to run the code in functorch\n",
        "    # for evaluating the Jacobian-vector product\n",
        "    func, params = ft.make_functional(net)\n",
        "\n",
        "    # removing requires gradient as it will not be used\n",
        "    # for the forward AD\n",
        "    for param in params:\n",
        "        param.requires_grad_(False)\n",
        "\n",
        "    # defining our optimizer\n",
        "    opt = ForwardSGD(func, functional_xent, params, lr=gamma, momentum = False, decay=lambda_)\n",
        "    # running the code for e epochs\n",
        "    epochs = 10\n",
        "    test_losses_fwd = []\n",
        "    for e in range(epochs):\n",
        "      # training\n",
        "      for i, (image, label) in enumerate(train_data_loader):\n",
        "        image, label = image.to(device), label.to(device)\n",
        "        params, loss = opt.step(image, label)\n",
        "      # evaluating on the test set\n",
        "      for i, (image, label) in enumerate(val_data_loader):\n",
        "        batch_loss = []\n",
        "        with torch.no_grad():\n",
        "          image, label = image.to(device), label.to(device)\n",
        "          val_loss = functional_xent(func, params, image, label)\n",
        "          batch_loss.append(val_loss.item())\n",
        "        test_losses.append(np.mean(batch_loss))\n",
        "        print(f\"Test loss in epoch {i+1}: {np.mean(batch_loss)}\")"
      ],
      "metadata": {
        "id": "b1G1P4OWwEk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conv Net"
      ],
      "metadata": {
        "id": "BKD8xerQrQ-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward gradient:"
      ],
      "metadata": {
        "id": "sEEwtxYCrW5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = ConvNet().to(device) # defining net\n",
        "\n",
        "# making the net functional to run the code in functorch\n",
        "# for evaluating the Jacobian-vector product\n",
        "func, params = ft.make_functional(net)\n",
        "\n",
        "# removing requires gradient as it will not be used\n",
        "# for the forward AD\n",
        "for param in params:\n",
        "    param.requires_grad_(False)\n",
        "\n",
        "# defining our optimizer\n",
        "opt = ForwardSGD(func, functional_xent, params, lr=2e-4, momentum = False, decay=1e-5)\n",
        "\n",
        "# running the code for e epochs\n",
        "losses_fwd = []\n",
        "epochs = 50\n",
        "test_losses = []\n",
        "for e in range(epochs):\n",
        "  # training\n",
        "  for i, (image, label) in enumerate(train_data_loader):\n",
        "    image, label = image.to(device), label.to(device)\n",
        "    params, loss = opt.step(image, label)\n",
        "    losses_fwd.append(loss.item())\n",
        "  # evaluating on the test set\n",
        "  for i, (image, label) in enumerate(test_data_loader):\n",
        "    batch_loss = []\n",
        "    with torch.no_grad():\n",
        "      image, label = image.to(device), label.to(device)\n",
        "      test_loss = functional_xent(func, params, image, label)\n",
        "      batch_loss.append(test_loss.item())\n",
        "    test_losses.append(np.mean(batch_loss))\n",
        "    print(f\"Test loss in epoch {i+1}: {np.mean(batch_loss)}\")"
      ],
      "metadata": {
        "id": "gfOzwDiqrX9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backpropagation:"
      ],
      "metadata": {
        "id": "byonpp14rafx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss() # loss function\n",
        "net = ConvNet().to(device) # defining net\n",
        "backprop = torch.optim.SGD(net.parameters(), lr=2e-4, weight_decay=1e-4) # normal SGD in torch\n",
        "\n",
        "epochs=50\n",
        "\n",
        "# storing losses\n",
        "losses = []\n",
        "test_losses = []\n",
        "for epoch in range(epochs):\n",
        "  # going over training set in batches\n",
        "  for i, (image, label) in enumerate(train_data_loader):\n",
        "    image, label = image.to(device), label.to(device)\n",
        "    backprop.zero_grad()\n",
        "    outputs = net(image)\n",
        "    loss = criterion(outputs, label)\n",
        "    loss.backward()\n",
        "    backprop.step()\n",
        "    losses.append(loss.item())\n",
        "  for i, (image, label) in enumerate(test_data_loader):\n",
        "    batch_loss = []\n",
        "    with torch.no_grad():\n",
        "      image, label = image.to(device), label.to(device)\n",
        "      test_loss = criterion(net(image), label)\n",
        "      batch_loss.append(test_loss.item())\n",
        "    test_losses.append(np.mean(batch_loss))\n",
        "    print(f\"Test loss in epoch {i+1}: {np.mean(batch_loss)}\")\n",
        "    "
      ],
      "metadata": {
        "id": "cNWURG-mrcCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparing results"
      ],
      "metadata": {
        "id": "R_fVJq9ardZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,2,figsize=(16,8))\n",
        "\n",
        "ax[0].plot(losses, color='r', label=\"Backprop\", alpha=.7)\n",
        "ax[0].set_xlabel(\"Iterations\")\n",
        "ax[0].set_ylabel(\"Loss\")\n",
        "ax[0].plot(losses_fwd, color='b', label='Forward gradient', alpha=.7)\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(test_losses, color='r', label=\"Backprop\", alpha=.7)\n",
        "ax[1].set_xlabel(\"Epochs\")\n",
        "ax[1].set_ylabel(\"Loss\")\n",
        "ax[1].plot(test_losses_fwd, color='b', label='Forward gradient', alpha=.7)\n",
        "ax[1].legend()"
      ],
      "metadata": {
        "id": "0blf7s25retQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
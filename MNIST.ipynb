{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Vvt5q3XaqbUc"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MNIST\n",
        "\n",
        "This notebook contains our experiements comparing forward gradient and backpropagation with neural networks and Convolutional nets for the MNIST dataset"
      ],
      "metadata": {
        "id": "SpgPiFcBm1hX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup"
      ],
      "metadata": {
        "id": "JmBQQnpWoJrd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnA7qJ4F78Gr",
        "outputId": "24c83c51-602f-4871-ce8d-3571aa18beb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cpu/torch-1.13.0.dev20220611%2Bcpu-cp37-cp37m-linux_x86_64.whl (190.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 190.7 MB 78 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.13.0.dev20220611+cpu which is incompatible.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.13.0.dev20220611+cpu which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.13.0.dev20220611+cpu which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.13.0.dev20220611+cpu\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting functorch\n",
            "  Downloading functorch-0.1.1-cp37-cp37m-manylinux1_x86_64.whl (21.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.4 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting torch<1.12,>=1.11\n",
            "  Downloading torch-1.11.0-cp37-cp37m-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 750.6 MB 11 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<1.12,>=1.11->functorch) (4.2.0)\n",
            "Installing collected packages: torch, functorch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.0.dev20220611+cpu\n",
            "    Uninstalling torch-1.13.0.dev20220611+cpu:\n",
            "      Successfully uninstalled torch-1.13.0.dev20220611+cpu\n",
            "Successfully installed functorch-0.1.1 torch-1.11.0\n",
            "--> Restarting colab instance\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'restart': True, 'status': 'ok'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Run once\n",
        "# CPU only: !pip install torch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0 -f https://download.pytorch.org/whl/cpu/torch_stable.html\n",
        "!pip install --pre torch -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html --upgrade\n",
        "!pip install functorch\n",
        "print(\"--> Restarting colab instance\") \n",
        "get_ipython().kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/benjaminrike1/forward_gradient"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w16dKpsQbsXH",
        "outputId": "862921e2-7fdd-4de6-ba9b-eed910f20fb0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'forward_gradient'...\n",
            "remote: Enumerating objects: 68, done.\u001b[K\n",
            "remote: Counting objects: 100% (68/68), done.\u001b[K\n",
            "remote: Compressing objects: 100% (54/54), done.\u001b[K\n",
            "remote: Total 68 (delta 25), reused 46 (delta 13), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (68/68), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd forward_gradient"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7a4RrKbmuKQ",
        "outputId": "0706b1dc-633f-464a-95b2-b84718294e43"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/forward_gradient\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngzkoQhx8SHI",
        "outputId": "62f1aabb-5ba5-4823-8fc2-9b9db67a59e9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f1685df1830>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "import functorch as ft\n",
        "\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from optim_functions import beale, rosenbrock\n",
        "from helpers import optimize\n",
        "from plot_helpers import plot_loss, plot_countour, plot_contour2\n",
        "from loss import functional_xent, softmax, clamp_probs, _xent\n",
        "from optimizers import ForwardSGD\n",
        "from models import Net, ConvNet, LogisticRegression\n",
        "\n",
        "torch.manual_seed(0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST"
      ],
      "metadata": {
        "id": "Vvt5q3XaqbUc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the data from torchvision."
      ],
      "metadata": {
        "id": "Vy2UKEhnqexi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "mnist_train = torchvision.datasets.MNIST(\n",
        "    '/tmp/data',\n",
        "    train=True, \n",
        "    download=True, \n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "train, val = torch.utils.data.random_split(mnist_train, [50000, 10000])\n",
        "\n",
        "train_data_loader = torch.utils.data.DataLoader(train, \n",
        "                                          batch_size=64, \n",
        "                                          shuffle=True)\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(val, \n",
        "                                          batch_size=64, \n",
        "                                          shuffle=True)\n",
        "\n",
        "mnist_test = torchvision.datasets.MNIST(\n",
        "    '/tmp/data',\n",
        "    train=False, \n",
        "    download=True, \n",
        "    transform=transform\n",
        ")\n",
        "test_data_loader = torch.utils.data.DataLoader(mnist_test, \n",
        "                                              batch_size=64,\n",
        "                                              shuffle=True)"
      ],
      "metadata": {
        "id": "fnfSMWT8qajP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network"
      ],
      "metadata": {
        "id": "xnoi5RW9qm5U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SGD"
      ],
      "metadata": {
        "id": "_qH5VOIWwAQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "TTxOWIqgqyDv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward gradient:"
      ],
      "metadata": {
        "id": "8GhqG9jAqy03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net().to(device) # defining net\n",
        "\n",
        "# making the net functional to run the code in functorch\n",
        "# for evaluating the Jacobian-vector product\n",
        "func, params = ft.make_functional(net)\n",
        "\n",
        "# removing requires gradient as it will not be used\n",
        "# for the forward AD\n",
        "for param in params:\n",
        "    param.requires_grad_(False)\n",
        "\n",
        "# defining our optimizer\n",
        "opt = ForwardSGD(func, functional_xent, params, lr=2e-4, momentum = False, decay=1e-5)\n",
        "\n",
        "# running the code for e epochs\n",
        "losses_fwd = []\n",
        "epochs = 50\n",
        "test_losses = []\n",
        "for e in range(epochs):\n",
        "  # training\n",
        "  for i, (image, label) in enumerate(train_data_loader):\n",
        "    image, label = image.to(device), label.to(device)\n",
        "    params, loss = opt.step(image, label)\n",
        "    losses_fwd.append(loss.item())\n",
        "    # evaluating on the test set\n",
        "    for i, (image, label) in enumerate(test_data_loader):\n",
        "      with torch.no_grad():\n",
        "        batch_loss = []\n",
        "        image, label = image.to(device), label.to(device)\n",
        "        test_loss = functional_xent(func, params, image, label)\n",
        "        batch_loss.append(test_loss.item())\n",
        "        \n",
        "    test_losses.append(np.mean(batch_loss))\n",
        "    print(f\"Test loss in epoch {i+1}: {test_losses[-1]}\")"
      ],
      "metadata": {
        "id": "D7BcefWeqh_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backpropagation:"
      ],
      "metadata": {
        "id": "G4WpUezcqr4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss() # loss function\n",
        "net = Net().to(device) # defining net\n",
        "backprop = torch.optim.SGD(net.parameters(), lr=2e-4) # normal SGD in torch\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(backprop, gamma=1e-4)\n",
        "\n",
        "# storing losses\n",
        "losses = []\n",
        "test_losses = []\n",
        "for epoch in range(epochs):\n",
        "  # going over training set in batches\n",
        "  for i, (image, label) in enumerate(train_data_loader):\n",
        "    image, label = image.to(device), label.to(device)\n",
        "    backprop.zero_grad()\n",
        "    outputs = net(image)\n",
        "    loss = criterion(outputs, label)\n",
        "    loss.backward()\n",
        "    backprop.step()\n",
        "    losses.append(loss.item())\n",
        "    for i, (image, label) in enumerate(test_data_loader):\n",
        "      with torch.no_grad():\n",
        "        batch_loss = []\n",
        "        image, label = image.to(device), label.to(device)\n",
        "        test_loss = criterion(net(image), label)\n",
        "        batch_loss.append(test_loss.item())\n",
        "      test_losses.append(np.mean(batch_loss))\n",
        "    print(f\"Test loss in epoch {i+1}: {(test_losses[-1])}\")\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "id": "Id8vMur6qrcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparing results"
      ],
      "metadata": {
        "id": "j7wyih5wrNzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,2,figsize=(16,8))\n",
        "\n",
        "ax[0].plot(losses, color='r', label=\"Backprop\", alpha=.7)\n",
        "ax[0].set_xlabel(\"Iterations\")\n",
        "ax[0].set_ylabel(\"Loss\")\n",
        "ax[0].plot(losses_fwd, color='b', label='Forward gradient', alpha=.7)\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(test_losses, color='r', label=\"Backprop\", alpha=.7)\n",
        "ax[1].set_xlabel(\"Epochs\")\n",
        "ax[1].set_ylabel(\"Loss\")\n",
        "ax[1].plot(test_losses_fwd, color='b', label='Forward gradient', alpha=.7)\n",
        "ax[1].legend()"
      ],
      "metadata": {
        "id": "d1Az9_GcrQC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning rate optimization\n",
        "\n",
        "The final search for learning rate is in a quite small interval as we earlier tried a wider search, but wanted to reduce the width to find a better optimum."
      ],
      "metadata": {
        "id": "ixUBfiPVvy3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# learning rate and decays search for forward gradient\n",
        "\n",
        "learning_rates = np.logspace(-5, -3, 6)\n",
        "decays = np.logspace(-6, -4, 3)\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "for gamma in learning_rates:\n",
        "  for lambda_ in decays:\n",
        "    net = Net().to(device) # defining net\n",
        "\n",
        "    # making the net functional to run the code in functorch\n",
        "    # for evaluating the Jacobian-vector product\n",
        "    func, params = ft.make_functional(net)\n",
        "\n",
        "    # removing requires gradient as it will not be used\n",
        "    # for the forward AD\n",
        "    for param in params:\n",
        "        param.requires_grad_(False)\n",
        "\n",
        "    # defining our optimizer\n",
        "    opt = ForwardSGD(func, functional_xent, params, lr=gamma, momentum = False, decay=lambda_)\n",
        "    # running the code for e epochs\n",
        "    epochs = 10\n",
        "    for e in range(epochs):\n",
        "      # training\n",
        "      for i, (image, label) in enumerate(train_data_loader):\n",
        "        image, label = image.to(device), label.to(device)\n",
        "        params, loss = opt.step(image, label)\n",
        "      # evaluating on the test set\n",
        "    test_losses = []\n",
        "    for i, (image, label) in enumerate(val_data_loader):\n",
        "      batch_loss = []\n",
        "      with torch.no_grad():\n",
        "        image, label = image.to(device), label.to(device)\n",
        "        val_loss = functional_xent(params, func,image, label)\n",
        "        batch_loss.append(val_loss.item())\n",
        "      test_losses.append(np.mean(batch_loss))\n",
        "    print(f\"Validation loss for lr = {format(gamma,'.6f')}, decay = {format(lambda_,'.6f')}: {np.mean(test_losses)}\")"
      ],
      "metadata": {
        "id": "b1G1P4OWwEk2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bb9c937-24fe-4dd6-92d8-8baf8f75b66c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss for lr = 1e-05, decay = 1e-06: 2.3034265436184636\n",
            "Validation loss for lr = 1e-05, decay = 1e-05: 2.3073570864975075\n",
            "Validation loss for lr = 1e-05, decay = 0.0001: 2.29786008786244\n",
            "Validation loss for lr = 2.5118864315095822e-05, decay = 1e-06: 2.303787557942093\n",
            "Validation loss for lr = 2.5118864315095822e-05, decay = 1e-05: 2.301249429678461\n",
            "Validation loss for lr = 2.5118864315095822e-05, decay = 0.0001: 2.301920254519031\n",
            "Validation loss for lr = 6.309573444801929e-05, decay = 1e-06: 2.30180081288526\n",
            "Validation loss for lr = 6.309573444801929e-05, decay = 1e-05: 2.3055311509758045\n",
            "Validation loss for lr = 6.309573444801929e-05, decay = 0.0001: 2.30460032687825\n",
            "Validation loss for lr = 0.00015848931924611142, decay = 1e-06: 2.2985980480339876\n",
            "Validation loss for lr = 0.00015848931924611142, decay = 1e-05: 2.30292820778622\n",
            "Validation loss for lr = 0.00015848931924611142, decay = 0.0001: 2.2999486255038315\n",
            "Validation loss for lr = 0.00039810717055349735, decay = 1e-06: 2.305992964726345\n",
            "Validation loss for lr = 0.00039810717055349735, decay = 1e-05: 2.303788230677319\n",
            "Validation loss for lr = 0.00039810717055349735, decay = 0.0001: 2.3025484236941973\n",
            "Validation loss for lr = 0.001, decay = 1e-06: 2.3073137717641843\n",
            "Validation loss for lr = 0.001, decay = 1e-05: 2.3075691484341956\n",
            "Validation loss for lr = 0.001, decay = 0.0001: 2.2957606315612793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conv Net"
      ],
      "metadata": {
        "id": "BKD8xerQrQ-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward gradient:"
      ],
      "metadata": {
        "id": "sEEwtxYCrW5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = ConvNet().to(device) # defining net\n",
        "\n",
        "# making the net functional to run the code in functorch\n",
        "# for evaluating the Jacobian-vector product\n",
        "func, params = ft.make_functional(net)\n",
        "\n",
        "# removing requires gradient as it will not be used\n",
        "# for the forward AD\n",
        "for param in params:\n",
        "    param.requires_grad_(False)\n",
        "\n",
        "# defining our optimizer\n",
        "opt = ForwardSGD(func, functional_xent, params, lr=2e-4, momentum = False, decay=1e-5)\n",
        "\n",
        "# running the code for e epochs\n",
        "losses_fwd = []\n",
        "epochs = 50\n",
        "test_losses = []\n",
        "for e in range(epochs):\n",
        "  # training\n",
        "  for i, (image, label) in enumerate(train_data_loader):\n",
        "    image, label = image.to(device), label.to(device)\n",
        "    params, loss = opt.step(image, label)\n",
        "    losses_fwd.append(loss.item())\n",
        "  # evaluating on the test set\n",
        "  for i, (image, label) in enumerate(test_data_loader):\n",
        "    batch_loss = []\n",
        "    with torch.no_grad():\n",
        "      image, label = image.to(device), label.to(device)\n",
        "      test_loss = functional_xent(func, params, image, label)\n",
        "      batch_loss.append(test_loss.item())\n",
        "    test_losses.append(np.mean(batch_loss))\n",
        "    print(f\"Test loss in epoch {i+1}: {np.mean(batch_loss)}\")"
      ],
      "metadata": {
        "id": "gfOzwDiqrX9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backpropagation:"
      ],
      "metadata": {
        "id": "byonpp14rafx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss() # loss function\n",
        "net = ConvNet().to(device) # defining net\n",
        "backprop = torch.optim.SGD(net.parameters(), lr=2e-4) # normal SGD in torch\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(backprop, gamma=1e-4)\n",
        "\n",
        "# storing losses\n",
        "losses = []\n",
        "test_losses = []\n",
        "for epoch in range(epochs):\n",
        "  # going over training set in batches\n",
        "  for i, (image, label) in enumerate(train_data_loader):\n",
        "    image, label = image.to(device), label.to(device)\n",
        "    backprop.zero_grad()\n",
        "    outputs = net(image)\n",
        "    loss = criterion(outputs, label)\n",
        "    loss.backward()\n",
        "    backprop.step()\n",
        "    losses.append(loss.item())\n",
        "    for i, (image, label) in enumerate(test_data_loader):\n",
        "      with torch.no_grad():\n",
        "        image, label = image.to(device), label.to(device)\n",
        "        test_loss = criterion(net(image), label)\n",
        "        batch_loss.append(test_loss.item())\n",
        "        test_losses.append(np.mean(batch_loss))\n",
        "        batch_loss = []\n",
        "    print(f\"Test loss in epoch {i+1}: {np.mean(batch_loss)}\")\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "id": "cNWURG-mrcCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparing results"
      ],
      "metadata": {
        "id": "R_fVJq9ardZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,2,figsize=(16,8))\n",
        "\n",
        "ax[0].plot(losses, color='r', label=\"Backprop\", alpha=.7)\n",
        "ax[0].set_xlabel(\"Iterations\")\n",
        "ax[0].set_ylabel(\"Loss\")\n",
        "ax[0].plot(losses_fwd, color='b', label='Forward gradient', alpha=.7)\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(test_losses, color='r', label=\"Backprop\", alpha=.7)\n",
        "ax[1].set_xlabel(\"Epochs\")\n",
        "ax[1].set_ylabel(\"Loss\")\n",
        "ax[1].plot(test_losses_fwd, color='b', label='Forward gradient', alpha=.7)\n",
        "ax[1].legend()"
      ],
      "metadata": {
        "id": "0blf7s25retQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        },
        "id": "XnA7qJ4F78Gr",
        "outputId": "89f6c60e-5bb9-47ac-8ced-12b0a9d11f1f"
      },
      "outputs": [],
      "source": [
        "# Run once\n",
        "# CPU only: !pip install torch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0 -f https://download.pytorch.org/whl/cpu/torch_stable.html\n",
        "!pip install --pre torch -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html --upgrade\n",
        "!pip install functorch\n",
        "print(\"--> Restarting colab instance\") \n",
        "get_ipython().kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngzkoQhx8SHI",
        "outputId": "d0bc2eae-2639-4c05-ddd9-ae324671416d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "import functorch as ft\n",
        "\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from optim_functions import beale, rosenbrock\n",
        "from helpers import optimize\n",
        "from plot_helpers import plot_loss, plot_countour, plot_contour2\n",
        "from loss import functional_xent, softmax, clamp_probs, _xent\n",
        "from optimizers import forwardSGD\n",
        "from models import Net, ConvNet, LogisticRegression\n",
        "\n",
        "torch.manual_seed(0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1SLRmv5R0Mr"
      },
      "source": [
        "## Test functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Gmyzic9q8_ZQ",
        "outputId": "0303a022-0769-41de-dad2-ae1490918694"
      },
      "outputs": [],
      "source": [
        "steps = 100\n",
        "torch.manual_seed(0)\n",
        "primal0 = torch.randn(1) # x input\n",
        "primal1 = torch.randn(1) # y input\n",
        "\n",
        "x = primal0.clone()\n",
        "x.requires_grad_()\n",
        "y = primal1.clone()\n",
        "y.requires_grad_()\n",
        "params = (x, y)\n",
        "\n",
        "loss_rev, grad_rev, params_rev = optimize(beale, params, steps, optimizer=\"SGD\", lr=0.05)\n",
        "plot_loss(loss_rev, steps)\n",
        "\n",
        "loss_fwd, grad_fwd, params_rev = optimize(beale, (primal0, primal1), steps, lr=0.03)\n",
        "plot_loss(loss_fwd, steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "MCsJQm3AwQCB",
        "outputId": "ac6bbcbc-bfde-4b34-b74e-933536c12de4"
      },
      "outputs": [],
      "source": [
        "steps = 100\n",
        "torch.manual_seed(42)\n",
        "primal0 = torch.randn(1) # x input\n",
        "primal1 = torch.randn(1) # y input\n",
        "\n",
        "x = primal0.clone()\n",
        "x.requires_grad_()\n",
        "y = primal1.clone()\n",
        "y.requires_grad_()\n",
        "params = (x, y)\n",
        "\n",
        "loss_fwd, grad_fwd, params_fwd = optimize(beale, (primal0, primal1), 1000, lr=0.01)\n",
        "plot_contour2(loss_fwd, params_fwd, beale, (-1,4), (-1,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "GIFxZTxPzgxm",
        "outputId": "64288cfe-3935-4b27-d527-c1a64175c42b"
      },
      "outputs": [],
      "source": [
        "loss_rev, grad_rev, params_rev = optimize(beale, params, 1000, optimizer=\"SGD\", lr=0.01)\n",
        "plot_contour2(loss_rev, params_rev, beale, (-1,4), (-1,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "HgsF-1ZP0I1S",
        "outputId": "2237597f-ff0e-4f93-93dc-ddfc987f7f9d"
      },
      "outputs": [],
      "source": [
        "steps = 100\n",
        "torch.manual_seed(9)\n",
        "primal0 = torch.randn(1) # x input\n",
        "primal1 = torch.randn(1) # y input\n",
        "\n",
        "x = primal0.clone()\n",
        "x.requires_grad_()\n",
        "y = primal1.clone()\n",
        "y.requires_grad_()\n",
        "params = (x, y)\n",
        "\n",
        "loss_fwd, grad_fwd, params_fwd = optimize(rosenbrock, (primal0, primal1), 25000, lr=5e-4)\n",
        "plot_contour2(loss_fwd, params_fwd, rosenbrock, (-1.1,1.1), (0,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "wAm6f0T40PPp",
        "outputId": "b1d5021b-745c-4345-85cd-5a588cad1628"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(9)\n",
        "primal0 = torch.randn(1) # x input\n",
        "primal1 = torch.randn(1) # y input\n",
        "\n",
        "x = primal0.clone()\n",
        "x.requires_grad_()\n",
        "y = primal1.clone()\n",
        "y.requires_grad_()\n",
        "params = (x, y)\n",
        "\n",
        "loss_rev, grad_rev, params_rev = optimize(rosenbrock, params, 25000, optimizer=\"SGD\", lr=5e-4)\n",
        "plot_contour2(loss_rev, params_rev, rosenbrock, (-1.1,1.1), (0,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIPX02yNaE7X"
      },
      "source": [
        "Reproducing data from figure 1 in the paper:\n",
        "\n",
        "Real gradient: [-3.434, -0.808] \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0Wk00ayaDra"
      },
      "outputs": [],
      "source": [
        "primals = (torch.tensor([1.5]), torch.tensor([-0.1]))\n",
        "tangents = []\n",
        "fwd_grads = []\n",
        "steps=10\n",
        "for i in range(steps):\n",
        "  tangent = (torch.randn(1), torch.randn(1))\n",
        "  f, jvp = ft.jvp(beale, primals, tangent)\n",
        "  fwd_grads.append([jvp.mul(t).item() for t in tangent])\n",
        "  tangents.append([t.item() for t in tangent])\n",
        "avg_grad = np.mean(fwd_grads, axis=0)\n",
        "std_grad = np.std(fwd_grads, axis=0)\n",
        "avg1 = avg_grad[0]\n",
        "avg2 = avg_grad[1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "PyFg7DZRcsBh",
        "outputId": "cce8f83b-2858-4a64-8c76-d8b76021ce5d"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "V = np.asarray(tangents)\n",
        "W = np.asarray(fwd_grads)\n",
        "origin = np.zeros((2,steps))# origin point\n",
        "optimal1, optimal2 = -3.434, -0.808\n",
        "plt.quiver(*origin, V[:,0], V[:,1], color='y', angles='xy', scale_units='xy', scale=1, label=\"Perturbation vectors\")\n",
        "plt.quiver(*origin, W[:,0], W[:,1], color='r', label=\"Forward gradients\", angles='xy', scale_units='xy', scale=3)\n",
        "plt.quiver(0,0, optimal1, optimal2, color='b', label=\"Real gradient\", angles='xy', scale_units='xy', scale=1)\n",
        "plt.quiver(0,0, avg1, avg2, color='g', label='Avg. forward gradient', angles='xy', scale_units='xy', scale=1)\n",
        "\n",
        "plt.legend()\n",
        "plt.xlim(-4, 2)\n",
        "plt.ylim(-4, 2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgxaYfsLeLXy"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36nSuL5gmzXh"
      },
      "source": [
        "#### Import MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vkbp5YYI4k2V"
      },
      "outputs": [],
      "source": [
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "mnist_train = torchvision.datasets.MNIST(\n",
        "    '/tmp/data',\n",
        "    train=True, \n",
        "    download=True, \n",
        "    transform=transform\n",
        ")\n",
        "train_data_loader = torch.utils.data.DataLoader(mnist_train, \n",
        "                                          batch_size=64, \n",
        "                                          shuffle=True)\n",
        "\n",
        "mnist_test = torchvision.datasets.MNIST(\n",
        "    '/tmp/data',\n",
        "    train=False, \n",
        "    download=True, \n",
        "    transform=transform\n",
        ")\n",
        "test_data_loader = torch.utils.data.DataLoader(mnist_test, \n",
        "                                              batch_size=64,\n",
        "                                              shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot4dT633yn5D"
      },
      "source": [
        "#### Helpers from fwdgrad\n",
        "\n",
        "(https://github.com/orobix/fwdgrad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbH44mqHXbdu"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "net = ConvNet().to(device)\n",
        "    \n",
        "func, params = ft.make_functional(net)\n",
        "\n",
        "for param in params:\n",
        "    param.requires_grad_(False)\n",
        "\n",
        "opt = forwardSGD(func, functional_xent, params, lr=2e-4, momentum = False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "ccQyUVkkYTrI",
        "outputId": "c1f89d5c-40c6-4cf6-c6e4-fcb3d26c322f"
      },
      "outputs": [],
      "source": [
        "losses_fwd = []\n",
        "epochs = 2\n",
        "for epoch in range(epochs):\n",
        "  for i, (image, label) in enumerate(train_data_loader):\n",
        "    image, label = image.to(device), label.to(device)\n",
        "    _, loss = opt.step(image, label)\n",
        "    losses_fwd.append(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnAUWAwQYacw"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "net = Net()\n",
        "backprop = torch.optim.SGD(net.parameters(), lr=2e-4, weight_decay=1e-4)\n",
        "epochs=2\n",
        "losses = []\n",
        "for epoch in range(epochs):\n",
        "  for i, (image, label) in enumerate(train_data_loader):\n",
        "    image, label = image.to(device), label.to(device)\n",
        "    backprop.zero_grad()\n",
        "    outputs = net(image)\n",
        "    loss = criterion(outputs, label)\n",
        "    loss.backward()\n",
        "    backprop.step()\n",
        "    losses.append(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "Fn6Y8ItnWQKA",
        "outputId": "9ae9959b-ed2e-46ed-c540-1bc84db5d4b8"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1,2,figsize=(16,8))\n",
        "\n",
        "ax[0].plot(losses, color='r', label=\"Backprop\", alpha=.7)\n",
        "ax[0].set_xlabel(\"Iterations\")\n",
        "ax[0].set_ylabel(\"Loss\")\n",
        "ax[0].plot(losses_fwd, color='b', label='Forward gradient', alpha=.7)\n",
        "ax[0].legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ26ET1Am7QC"
      },
      "source": [
        "#### Define model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juo1icwq0eK9"
      },
      "source": [
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "WS-JFb9_9gyA",
        "outputId": "b17228bf-6f1d-409a-f9db-575e61e324b3"
      },
      "outputs": [],
      "source": [
        "from timeit import default_timer\n",
        "\n",
        "# Constanst\n",
        "EPOCHS = 5\n",
        "#lr = 0.005\n",
        "learning_rates = 10**np.random.uniform(-5, 0, size=6)\n",
        "\n",
        "\n",
        "model = LogisticRegression(784, 10).double()\n",
        "model.to('cuda')\n",
        "# model.train()\n",
        "\n",
        "# Define baseline\n",
        "baseline_model = LogisticRegression(784, 10).double()\n",
        "baseline_model.to('cuda')\n",
        "baseline_model.load_state_dict(model.state_dict())\n",
        "\n",
        "baseline_criterion = torch.nn.CrossEntropyLoss()\n",
        "baseline_optimizer = torch.optim.SGD(baseline_model.parameters(), lr=lr)\n",
        "\n",
        "# Creates version of the model that can be invoked like a function\n",
        "func, params, buffers = ft.make_functional_with_buffers(model)\n",
        "\n",
        "def criterion(params, fmodel, input, target):\n",
        "    y = fmodel(params, buffers, input)\n",
        "    return _xent(y, target)\n",
        "\n",
        "baseline_losses, baseline_accs = [[] for _ in learning_rates], [[] for _ in learning_rates]\n",
        "losses, accs = [[] for _ in learning_rates], [[] for _ in learning_rates]\n",
        "\n",
        "\n",
        "for n, lr in enumerate(learning_rates):\n",
        "  print(f'Using learning rate: {lr}')\n",
        "  for epoch in range(EPOCHS):\n",
        "    t = default_timer()\n",
        "      \n",
        "    for i, (image, label) in enumerate(train_data_loader):\n",
        "      #if i == 10:\n",
        "      #  break\n",
        "      image = image.to('cuda')\n",
        "      label = label.to('cuda')\n",
        "\n",
        "      #print('Model parameters:')\n",
        "      #print(list(params))\n",
        "      #print('Baseline parameters:')\n",
        "      #print(list(baseline_model.parameters()))\n",
        "      \n",
        "\n",
        "      # Update parameters manually\n",
        "      # Retrieve tangents for each parameter (for each forward pass)\n",
        "      tangents = tuple([torch.rand_like(p, device='cuda') for p in params])\n",
        "      \n",
        "      # Partial takes a function, and arguments to the function.\n",
        "      # The resulting function has only the parameters not specified in the partial call\n",
        "      # In our case, the model parameters (params)\n",
        "      f = partial(\n",
        "          criterion,\n",
        "          fmodel=func,\n",
        "          input=image,\n",
        "          target=label\n",
        "      )\n",
        "\n",
        "      # Calculate f and jvp\n",
        "      loss, jvp = ft.jvp(f, (params, ), (tangents, ))\n",
        "\n",
        "      gradients = [jvp.mul(tangent) for tangent in tangents]\n",
        "      with torch.no_grad():\n",
        "        for (g, param), target_param in zip(zip(gradients, params), model.parameters()):\n",
        "          new_param = param.sub_(lr * g)\n",
        "          target_param.copy_(new_param)\n",
        "      \n",
        "      # Update baseline\n",
        "      \n",
        "      baseline_optimizer.zero_grad()\n",
        "      output = baseline_model(image)\n",
        "      \n",
        "      baseline_loss = baseline_criterion(output.double(), F.one_hot(label, num_classes=10).double())\n",
        "      baseline_loss.backward()\n",
        "\n",
        "      # print(f'Backward loss: {baseline_loss.item():.2f}; Forward loss: {loss.item():.2f}')\n",
        "      baseline_optimizer.step()\n",
        "\n",
        "      #for g, b in zip(gradients, baseline_model.parameters()):\n",
        "      #  print('Forward gradient:')\n",
        "      #  print(g)\n",
        "      #  print('Backward gradient')\n",
        "      #  print(b.grad)\n",
        "      #  print('--------------------------------------------------')\n",
        "\n",
        "      # Gradient diff (in mse loss)\n",
        "      \n",
        "\n",
        "    # Should use validation set, not test set! \n",
        "    baseline_losses[n].append(baseline_loss.item())\n",
        "    baseline_accs[n].append(compute_accuracy(baseline_model))\n",
        "    losses[n].append(loss.item())\n",
        "    accs[n].append(compute_accuracy(model))\n",
        "\n",
        "    t = default_timer() - t\n",
        "    print(f'Epoch {epoch + 1} finished in {t:.2f} seconds with loss: {loss}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lfPKNc4Vx5L4",
        "outputId": "0fc30f99-c702-4728-905c-6014cac3d119"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axs = plt.subplots(3, 2, sharex=True, sharey=True, figsize=(15, 12))\n",
        "axs = axs.flatten()\n",
        "X = np.asarray(range(5))\n",
        "for i, ax in enumerate(axs):\n",
        "  sns.lineplot(X, accs[i], ax=ax)\n",
        "  sns.lineplot(X, baseline_accs[i], ax=ax)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUkT5w5xEiJZ",
        "outputId": "48661ace-9df1-482c-977d-d44f17da8e5d"
      },
      "outputs": [],
      "source": [
        "# Model test\n",
        "def compute_accuracy(model, batch_size=64):\n",
        "  correct = 0\n",
        "  for image, label in test_data_loader:\n",
        "    image = image.to('cuda')\n",
        "    label = label.to('cuda')\n",
        "    pred = model(image).argmax(dim=1)\n",
        "    correct += int((pred == label).sum())\n",
        "\n",
        "  return correct / len(test_data_loader) / batch_size\n",
        "\n",
        "compute_accuracy(model, 64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw9YJQpM8zOu",
        "outputId": "79042538-e8d1-479b-80ed-0d96614567c9"
      },
      "outputs": [],
      "source": [
        "# Baseline test\n",
        "compute_accuracy(baseline_model, 64)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "forward_grad_ben",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
